{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from game import game, random_connection_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Memb = 400\n",
    "newg = random_connection_game(9,  Memb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14 -0.03 -0.10 0.00 0.17 -0.06 -0.02 0.06 0.00 \n",
      "-0.03 0.07 0.00 0.10 -0.06 0.05 -0.06 0.03 0.10 \n",
      "-0.10 0.00 -0.10 0.03 0.00 -0.05 0.03 0.02 0.00 \n",
      "0.00 0.10 0.03 0.05 0.02 0.00 0.00 0.05 0.08 \n",
      "0.17 -0.06 0.00 0.02 -0.09 -0.09 -0.04 -0.16 -0.05 \n",
      "-0.06 0.05 -0.05 0.00 -0.09 0.06 0.03 -0.03 0.00 \n",
      "-0.02 -0.06 0.03 0.00 -0.04 0.03 0.00 0.17 -0.10 \n",
      "0.06 0.03 0.02 0.05 -0.16 -0.03 0.17 -0.16 -0.03 \n",
      "0.00 0.10 0.00 0.08 -0.05 0.00 -0.10 -0.03 0.23 \n"
     ]
    }
   ],
   "source": [
    "newg.print_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newg.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08 0.00 0.03 0.00 0.00 -0.12 0.00 -0.10 -0.05 \n",
      "0.00 0.07 0.00 -0.12 0.00 -0.02 -0.03 -0.10 0.07 \n",
      "0.03 0.00 0.09 0.00 -0.04 -0.04 0.04 -0.02 -0.11 \n",
      "0.00 -0.12 0.00 -0.35 0.12 0.00 -0.06 0.06 -0.03 \n",
      "0.00 0.00 -0.04 0.12 -0.18 0.03 -0.06 0.00 -0.04 \n",
      "-0.12 -0.02 -0.04 0.00 0.03 -0.06 0.05 0.00 -0.06 \n",
      "0.00 -0.03 0.04 -0.06 -0.06 0.05 0.05 0.00 0.00 \n",
      "-0.10 -0.10 -0.02 0.06 0.00 0.00 0.00 -0.05 0.03 \n",
      "-0.05 0.07 -0.11 -0.03 -0.04 -0.06 0.00 0.03 -0.06 \n"
     ]
    }
   ],
   "source": [
    "newg.print_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160081,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newg.state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomAgent(state, Memb = 25):\n",
    "    move = random.randint(3)\n",
    "    spin_1 = random.randint(Memb)\n",
    "    spin_2 = random.randint(4)\n",
    "    return (move, spin_1, spin_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dqn_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset!\n",
    "Memb = 25\n",
    "statesize = Memb*Memb*1+9*9\n",
    "actionsize = 3*Memb*4\n",
    "randomAgent0 = lambda state: randomAgent(state, Memb)\n",
    "#env = game(Hint, Memb = Memb)\n",
    "smartagent0 = Agent(statesize, actionsize, seed = 1,\n",
    "                    embedding_size = Memb, neighbours = 4,\n",
    "                   nu = [32, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.25 3 (2, 294, 1) 55\n"
     ]
    }
   ],
   "source": [
    "env = random_connection_game(6,  Memb)\n",
    "\n",
    "#\n",
    "state = env.state\n",
    "fig, ax = env.plot()\n",
    "score = 0.0\n",
    "actions = []\n",
    "for j in range(2000):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    action = smartagent0.act(state, eps = 1e-1)\n",
    "    state, reward, done =env.step(action)\n",
    "    score += reward\n",
    "    actions.append((*action, reward))\n",
    "\n",
    "    fig, ax = env.plot()\n",
    "    plt.show()\n",
    "    print(score, env.N, action, env.terms_left)\n",
    "    if done >0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "def dqn(agent, n_episodes=6000, max_t=500, \n",
    "        eps_start=1.0, eps_end=0.01, eps_decay=0.9995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            action_tr = action[0]+action[1]*3+action[2]*3*Memb\n",
    "            agent.step(state, action_tr, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score, epsilo: {:.2f}, {}'.format(i_episode, np.mean(scores_window), eps))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_online.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score, epsilo: -6.80, 0.951217530242334\n",
      "Episode 200\tAverage Score, epsilo: -6.79, 0.9048147898403269\n",
      "Episode 300\tAverage Score, epsilo: -6.79, 0.8606756897186528\n",
      "Episode 400\tAverage Score, epsilo: -6.80, 0.8186898039137951\n",
      "Episode 500\tAverage Score, epsilo: -6.83, 0.7787520933134615\n",
      "Episode 600\tAverage Score, epsilo: -6.83, 0.7407626428726788\n",
      "Episode 700\tAverage Score, epsilo: -6.84, 0.7046264116491338\n",
      "Episode 800\tAverage Score, epsilo: -6.83, 0.6702529950324074\n",
      "Episode 900\tAverage Score, epsilo: -6.81, 0.637556398572254\n",
      "Episode 1000\tAverage Score, epsilo: -6.88, 0.606454822840097\n",
      "Episode 1100\tAverage Score, epsilo: -6.83, 0.5768704587855094\n",
      "Episode 1200\tAverage Score, epsilo: -6.84, 0.548729293075715\n",
      "Episode 1266\tAverage Score: -6.91"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0269e0209d87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmartagent0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-f9ddc2704e6c>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(agent, n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0maction_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mMemb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/mios/game_embeddings/model/dqn_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# If enough samples are available in memory, get random subset and learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/mios/game_embeddings/model/dqn_agent.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/envs/pytorch/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = dqn(smartagent0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "score = 0\n",
    "for t in range(1):\n",
    "    action = smartagent0.act(state, 0.1)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    action_tr = action[0]+action[1]*3+action[2]*3*Memb\n",
    "    smartagent0.step(state, action_tr, reward, next_state, done)\n",
    "    state = next_state\n",
    "    score += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
