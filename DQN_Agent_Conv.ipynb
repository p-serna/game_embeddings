{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from game import game, random_connection_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Memb = 400\n",
    "newg = random_connection_game(9,  Memb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 -0.08 -0.04 -0.06 0.00 0.13 -0.14 0.00 0.10 \n",
      "-0.08 -0.11 -0.06 0.08 0.05 0.05 0.07 -0.07 0.00 \n",
      "-0.04 -0.06 0.13 0.03 0.06 0.12 -0.08 -0.10 -0.05 \n",
      "-0.06 0.08 0.03 0.04 -0.02 -0.16 0.00 0.00 0.00 \n",
      "0.00 0.05 0.06 -0.02 -0.09 0.00 -0.04 0.07 -0.08 \n",
      "0.13 0.05 0.12 -0.16 0.00 -0.08 -0.05 0.00 0.04 \n",
      "-0.14 0.07 -0.08 0.00 -0.04 -0.05 -0.09 0.06 -0.12 \n",
      "0.00 -0.07 -0.10 0.00 0.07 0.00 0.06 0.31 0.15 \n",
      "0.10 0.00 -0.05 0.00 -0.08 0.04 -0.12 0.15 0.00 \n"
     ]
    }
   ],
   "source": [
    "newg.print_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 -0.08 -0.04 -0.06 0.00 0.13 -0.14 0.00 0.10 \n",
      "-0.08 -0.11 -0.06 0.08 0.05 0.05 0.07 -0.07 0.00 \n",
      "-0.04 -0.06 0.13 0.03 0.06 0.12 -0.08 -0.10 -0.05 \n",
      "-0.06 0.08 0.03 0.04 -0.02 -0.16 0.00 0.00 0.00 \n",
      "0.00 0.05 0.06 -0.02 -0.09 0.00 -0.04 0.07 -0.08 \n",
      "0.13 0.05 0.12 -0.16 0.00 -0.08 -0.05 0.00 0.04 \n",
      "-0.14 0.07 -0.08 0.00 -0.04 -0.05 -0.09 0.06 -0.12 \n",
      "0.00 -0.07 -0.10 0.00 0.07 0.00 0.06 0.31 0.15 \n",
      "0.10 0.00 -0.05 0.00 -0.08 0.04 -0.12 0.15 0.00 \n"
     ]
    }
   ],
   "source": [
    "newg.print_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 400, 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newg.state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomAgent(state, Memb = 25):\n",
    "    move = random.randint(3)\n",
    "    spin_1 = random.randint(Memb)\n",
    "    spin_2 = random.randint(4)\n",
    "    return (move, spin_1, spin_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dqn_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import QNetwork_Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset!\n",
    "Memb = 4*4\n",
    "statesize = Memb*Memb*1+9*9\n",
    "actionsize = 3*Memb*4\n",
    "randomAgent0 = lambda state: randomAgent(state, Memb)\n",
    "#env = game(Hint, Memb = Memb)\n",
    "smartagent0 = Agent(statesize, actionsize, seed = 1,\n",
    "                    embedding_size = Memb, neighbours = 4,\n",
    "                   nu = [32, 64, 128, 128, 64], model = QNetwork_Conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = random_connection_game(9,  Memb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 16, 16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.8 4 (0, 11, 0) 10\n"
     ]
    }
   ],
   "source": [
    "env = random_connection_game(4,  Memb)\n",
    "\n",
    "#\n",
    "state = env.state\n",
    "fig, ax = env.plot()\n",
    "score = 0.0\n",
    "actions = []\n",
    "for j in range(2000):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    action = smartagent0.act(state, eps = 1.0)\n",
    "    state, reward, done =env.step(action)\n",
    "    score += reward\n",
    "    actions.append((*action, reward))\n",
    "\n",
    "    fig, ax = env.plot()\n",
    "    plt.show()\n",
    "    print(score, env.N, action, env.terms_left)\n",
    "    if done >0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "def dqn(agent, n_episodes=2000, max_t=2000, \n",
    "        eps_start=1.0, eps_end=0.01, eps_decay=0.9995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            action_tr = action[0]+action[1]*3+action[2]*3*Memb\n",
    "            agent.step(state, action_tr, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score, eps: {:.2f}, {:.2f}'.format(i_episode, np.mean(scores_window), eps))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_online.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 64, 64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(state,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 64, 64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack((np.expand_dims(state,0),np.expand_dims(state,0))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score, eps: -3.97, 0.95\n",
      "Episode 200\tAverage Score, eps: -3.97, 0.90\n",
      "Episode 300\tAverage Score, eps: -3.99, 0.86\n",
      "Episode 400\tAverage Score, eps: -3.90, 0.82\n",
      "Episode 500\tAverage Score, eps: -3.93, 0.78\n",
      "Episode 600\tAverage Score, eps: -3.95, 0.74\n",
      "Episode 700\tAverage Score, eps: -3.87, 0.70\n",
      "Episode 800\tAverage Score, eps: -3.92, 0.67\n",
      "Episode 848\tAverage Score: -3.91"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0269e0209d87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmartagent0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-c588e9b55303>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(agent, n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0maction_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mMemb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/mios/game_embeddings/model/dqn_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# If enough samples are available in memory, get random subset and learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/mios/game_embeddings/model/dqn_agent.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/envs/pytorch/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_vhstack_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = dqn(smartagent0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "score = 0\n",
    "for t in range(1):\n",
    "    action = smartagent0.act(state, 0.1)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    action_tr = action[0]+action[1]*3+action[2]*3*Memb\n",
    "    smartagent0.step(state, action_tr, reward, next_state, done)\n",
    "    state = next_state\n",
    "    score += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartagent0.memory.sample()[0].cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
